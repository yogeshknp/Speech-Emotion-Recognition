{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sexual-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa # used for sound and musical analysis\n",
    "import soundfile # handling audio files\n",
    "import os, glob # access files from folders, glob is used for acessing particular parsed files using filenames\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41056d4e",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "We are extracting three types of features for our audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    \n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate    \n",
    "        result=np.array([])\n",
    "        \n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        \n",
    "        stft=np.abs(librosa.stft(X))\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        \n",
    "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thorough-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "emotions_to_be_observed=['happy','sad','angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabulous-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    i=0\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(r\"B:\\Semesters\\7th sem\\BTP\\Datasets\\Emotion data\\Actor_*\\\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in emotions_to_be_observed:\n",
    "            continue\n",
    "        feature=extract_feature(file)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "        print(i,emotion)\n",
    "        i=i+1\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amber-avenue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 happy\n",
      "1 happy\n",
      "2 happy\n",
      "3 happy\n",
      "4 happy\n",
      "5 happy\n",
      "6 happy\n",
      "7 happy\n",
      "8 sad\n",
      "9 sad\n",
      "10 sad\n",
      "11 sad\n",
      "12 sad\n",
      "13 sad\n",
      "14 sad\n",
      "15 sad\n",
      "16 angry\n",
      "17 angry\n",
      "18 angry\n",
      "19 angry\n",
      "20 angry\n",
      "21 angry\n",
      "22 angry\n",
      "23 angry\n",
      "24 happy\n",
      "25 happy\n",
      "26 happy\n",
      "27 happy\n",
      "28 happy\n",
      "29 happy\n",
      "30 happy\n",
      "31 happy\n",
      "32 sad\n",
      "33 sad\n",
      "34 sad\n",
      "35 sad\n",
      "36 sad\n",
      "37 sad\n",
      "38 sad\n",
      "39 sad\n",
      "40 angry\n",
      "41 angry\n",
      "42 angry\n",
      "43 angry\n",
      "44 angry\n",
      "45 angry\n",
      "46 angry\n",
      "47 angry\n",
      "48 happy\n",
      "49 happy\n",
      "50 happy\n",
      "51 happy\n",
      "52 happy\n",
      "53 happy\n",
      "54 happy\n",
      "55 happy\n",
      "56 sad\n",
      "57 sad\n",
      "58 sad\n",
      "59 sad\n",
      "60 sad\n",
      "61 sad\n",
      "62 sad\n",
      "63 sad\n",
      "64 angry\n",
      "65 angry\n",
      "66 angry\n",
      "67 angry\n",
      "68 angry\n",
      "69 angry\n",
      "70 angry\n",
      "71 angry\n",
      "72 happy\n",
      "73 happy\n",
      "74 happy\n",
      "75 happy\n",
      "76 happy\n",
      "77 happy\n",
      "78 happy\n",
      "79 happy\n",
      "80 sad\n",
      "81 sad\n",
      "82 sad\n",
      "83 sad\n",
      "84 sad\n",
      "85 sad\n",
      "86 sad\n",
      "87 sad\n",
      "88 angry\n",
      "89 angry\n",
      "90 angry\n",
      "91 angry\n",
      "92 angry\n",
      "93 angry\n",
      "94 angry\n",
      "95 angry\n",
      "96 happy\n",
      "97 happy\n",
      "98 happy\n",
      "99 happy\n",
      "100 happy\n",
      "101 happy\n",
      "102 happy\n",
      "103 happy\n",
      "104 sad\n",
      "105 sad\n",
      "106 sad\n",
      "107 sad\n",
      "108 sad\n",
      "109 sad\n",
      "110 sad\n",
      "111 sad\n",
      "112 angry\n",
      "113 angry\n",
      "114 angry\n",
      "115 angry\n",
      "116 angry\n",
      "117 angry\n",
      "118 angry\n",
      "119 angry\n",
      "120 happy\n",
      "121 happy\n",
      "122 happy\n",
      "123 happy\n",
      "124 happy\n",
      "125 happy\n",
      "126 happy\n",
      "127 happy\n",
      "128 sad\n",
      "129 sad\n",
      "130 sad\n",
      "131 sad\n",
      "132 sad\n",
      "133 sad\n",
      "134 sad\n",
      "135 sad\n",
      "136 angry\n",
      "137 angry\n",
      "138 angry\n",
      "139 angry\n",
      "140 angry\n",
      "141 angry\n",
      "142 angry\n",
      "143 angry\n",
      "144 happy\n",
      "145 happy\n",
      "146 happy\n",
      "147 happy\n",
      "148 happy\n",
      "149 happy\n",
      "150 happy\n",
      "151 happy\n",
      "152 sad\n",
      "153 sad\n",
      "154 sad\n",
      "155 sad\n",
      "156 sad\n",
      "157 sad\n",
      "158 sad\n",
      "159 sad\n",
      "160 angry\n",
      "161 angry\n",
      "162 angry\n",
      "163 angry\n",
      "164 angry\n",
      "165 angry\n",
      "166 angry\n",
      "167 angry\n",
      "168 happy\n",
      "169 happy\n",
      "170 happy\n",
      "171 happy\n",
      "172 happy\n",
      "173 happy\n",
      "174 happy\n",
      "175 happy\n",
      "176 sad\n",
      "177 sad\n",
      "178 sad\n",
      "179 sad\n",
      "180 sad\n",
      "181 sad\n",
      "182 sad\n",
      "183 sad\n",
      "184 angry\n",
      "185 angry\n",
      "186 angry\n",
      "187 angry\n",
      "188 angry\n",
      "189 angry\n",
      "190 angry\n",
      "191 angry\n",
      "192 happy\n",
      "193 happy\n",
      "194 happy\n",
      "195 happy\n",
      "196 happy\n",
      "197 happy\n",
      "198 happy\n",
      "199 happy\n",
      "200 sad\n",
      "201 sad\n",
      "202 sad\n",
      "203 sad\n",
      "204 sad\n",
      "205 sad\n",
      "206 sad\n",
      "207 sad\n",
      "208 angry\n",
      "209 angry\n",
      "210 angry\n",
      "211 angry\n",
      "212 angry\n",
      "213 angry\n",
      "214 angry\n",
      "215 angry\n",
      "216 happy\n",
      "217 happy\n",
      "218 happy\n",
      "219 happy\n",
      "220 happy\n",
      "221 happy\n",
      "222 happy\n",
      "223 happy\n",
      "224 sad\n",
      "225 sad\n",
      "226 sad\n",
      "227 sad\n",
      "228 sad\n",
      "229 sad\n",
      "230 sad\n",
      "231 sad\n",
      "232 angry\n",
      "233 angry\n",
      "234 angry\n",
      "235 angry\n",
      "236 angry\n",
      "237 angry\n",
      "238 angry\n",
      "239 angry\n",
      "240 happy\n",
      "241 happy\n",
      "242 happy\n",
      "243 happy\n",
      "244 happy\n",
      "245 happy\n",
      "246 happy\n",
      "247 happy\n",
      "248 sad\n",
      "249 sad\n",
      "250 sad\n",
      "251 sad\n",
      "252 sad\n",
      "253 sad\n",
      "254 sad\n",
      "255 sad\n",
      "256 angry\n",
      "257 angry\n",
      "258 angry\n",
      "259 angry\n",
      "260 angry\n",
      "261 angry\n",
      "262 angry\n",
      "263 angry\n",
      "264 happy\n",
      "265 happy\n",
      "266 happy\n",
      "267 happy\n",
      "268 happy\n",
      "269 happy\n",
      "270 happy\n",
      "271 happy\n",
      "272 sad\n",
      "273 sad\n",
      "274 sad\n",
      "275 sad\n",
      "276 sad\n",
      "277 sad\n",
      "278 sad\n",
      "279 sad\n",
      "280 angry\n",
      "281 angry\n",
      "282 angry\n",
      "283 angry\n",
      "284 angry\n",
      "285 angry\n",
      "286 angry\n",
      "287 angry\n",
      "288 happy\n",
      "289 happy\n",
      "290 happy\n",
      "291 happy\n",
      "292 happy\n",
      "293 happy\n",
      "294 happy\n",
      "295 happy\n",
      "296 sad\n",
      "297 sad\n",
      "298 sad\n",
      "299 sad\n",
      "300 sad\n",
      "301 sad\n",
      "302 sad\n",
      "303 sad\n",
      "304 angry\n",
      "305 angry\n",
      "306 angry\n",
      "307 angry\n",
      "308 angry\n",
      "309 angry\n",
      "310 angry\n",
      "311 angry\n",
      "312 happy\n",
      "313 happy\n",
      "314 happy\n",
      "315 happy\n",
      "316 happy\n",
      "317 happy\n",
      "318 happy\n",
      "319 happy\n",
      "320 sad\n",
      "321 sad\n",
      "322 sad\n",
      "323 sad\n",
      "324 sad\n",
      "325 sad\n",
      "326 sad\n",
      "327 sad\n",
      "328 angry\n",
      "329 angry\n",
      "330 angry\n",
      "331 angry\n",
      "332 angry\n",
      "333 angry\n",
      "334 angry\n",
      "335 angry\n",
      "336 happy\n",
      "337 happy\n",
      "338 happy\n",
      "339 happy\n",
      "340 happy\n",
      "341 happy\n",
      "342 happy\n",
      "343 happy\n",
      "344 sad\n",
      "345 sad\n",
      "346 sad\n",
      "347 sad\n",
      "348 sad\n",
      "349 sad\n",
      "350 sad\n",
      "351 sad\n",
      "352 angry\n",
      "353 angry\n",
      "354 angry\n",
      "355 angry\n",
      "356 angry\n",
      "357 angry\n",
      "358 angry\n",
      "359 angry\n",
      "360 happy\n",
      "361 happy\n",
      "362 happy\n",
      "363 happy\n",
      "364 happy\n",
      "365 happy\n",
      "366 happy\n",
      "367 happy\n",
      "368 sad\n",
      "369 sad\n",
      "370 sad\n",
      "371 sad\n",
      "372 sad\n",
      "373 sad\n",
      "374 sad\n",
      "375 sad\n",
      "376 angry\n",
      "377 angry\n",
      "378 angry\n",
      "379 angry\n",
      "380 angry\n",
      "381 angry\n",
      "382 angry\n",
      "383 angry\n",
      "384 happy\n",
      "385 happy\n",
      "386 happy\n",
      "387 happy\n",
      "388 happy\n",
      "389 happy\n",
      "390 happy\n",
      "391 happy\n",
      "392 sad\n",
      "393 sad\n",
      "394 sad\n",
      "395 sad\n",
      "396 sad\n",
      "397 sad\n",
      "398 sad\n",
      "399 sad\n",
      "400 angry\n",
      "401 angry\n",
      "402 angry\n",
      "403 angry\n",
      "404 angry\n",
      "405 angry\n",
      "406 angry\n",
      "407 angry\n",
      "408 happy\n",
      "409 happy\n",
      "410 happy\n",
      "411 happy\n",
      "412 happy\n",
      "413 happy\n",
      "414 happy\n",
      "415 happy\n",
      "416 sad\n",
      "417 sad\n",
      "418 sad\n",
      "419 sad\n",
      "420 sad\n",
      "421 sad\n",
      "422 sad\n",
      "423 sad\n",
      "424 angry\n",
      "425 angry\n",
      "426 angry\n",
      "427 angry\n",
      "428 angry\n",
      "429 angry\n",
      "430 angry\n",
      "431 angry\n",
      "432 happy\n",
      "433 happy\n",
      "434 happy\n",
      "435 happy\n",
      "436 happy\n",
      "437 happy\n",
      "438 happy\n",
      "439 happy\n",
      "440 sad\n",
      "441 sad\n",
      "442 sad\n",
      "443 sad\n",
      "444 sad\n",
      "445 sad\n",
      "446 sad\n",
      "447 sad\n",
      "448 angry\n",
      "449 angry\n",
      "450 angry\n",
      "451 angry\n",
      "452 angry\n",
      "453 angry\n",
      "454 angry\n",
      "455 angry\n",
      "456 happy\n",
      "457 happy\n",
      "458 happy\n",
      "459 happy\n",
      "460 happy\n",
      "461 happy\n",
      "462 happy\n",
      "463 happy\n",
      "464 sad\n",
      "465 sad\n",
      "466 sad\n",
      "467 sad\n",
      "468 sad\n",
      "469 sad\n",
      "470 sad\n",
      "471 sad\n",
      "472 angry\n",
      "473 angry\n",
      "474 angry\n",
      "475 angry\n",
      "476 angry\n",
      "477 angry\n",
      "478 angry\n",
      "479 angry\n",
      "480 happy\n",
      "481 happy\n",
      "482 happy\n",
      "483 happy\n",
      "484 happy\n",
      "485 happy\n",
      "486 happy\n",
      "487 happy\n",
      "488 sad\n",
      "489 sad\n",
      "490 sad\n",
      "491 sad\n",
      "492 sad\n",
      "493 sad\n",
      "494 sad\n",
      "495 sad\n",
      "496 angry\n",
      "497 angry\n",
      "498 angry\n",
      "499 angry\n",
      "500 angry\n",
      "501 angry\n",
      "502 angry\n",
      "503 angry\n",
      "504 happy\n",
      "505 happy\n",
      "506 happy\n",
      "507 happy\n",
      "508 happy\n",
      "509 happy\n",
      "510 happy\n",
      "511 happy\n",
      "512 sad\n",
      "513 sad\n",
      "514 sad\n",
      "515 sad\n",
      "516 sad\n",
      "517 sad\n",
      "518 sad\n",
      "519 sad\n",
      "520 angry\n",
      "521 angry\n",
      "522 angry\n",
      "523 angry\n",
      "524 angry\n",
      "525 angry\n",
      "526 angry\n",
      "527 angry\n",
      "528 happy\n",
      "529 happy\n",
      "530 happy\n",
      "531 happy\n",
      "532 happy\n",
      "533 happy\n",
      "534 happy\n",
      "535 happy\n",
      "536 sad\n",
      "537 sad\n",
      "538 sad\n",
      "539 sad\n",
      "540 sad\n",
      "541 sad\n",
      "542 sad\n",
      "543 sad\n",
      "544 angry\n",
      "545 angry\n",
      "546 angry\n",
      "547 angry\n",
      "548 angry\n",
      "549 angry\n",
      "550 angry\n",
      "551 angry\n",
      "552 happy\n",
      "553 happy\n",
      "554 happy\n",
      "555 happy\n",
      "556 happy\n",
      "557 happy\n",
      "558 happy\n",
      "559 happy\n",
      "560 sad\n",
      "561 sad\n",
      "562 sad\n",
      "563 sad\n",
      "564 sad\n",
      "565 sad\n",
      "566 sad\n",
      "567 sad\n",
      "568 angry\n",
      "569 angry\n",
      "570 angry\n",
      "571 angry\n",
      "572 angry\n",
      "573 angry\n",
      "574 angry\n",
      "575 angry\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = load_data(test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe0c7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 116)\n"
     ]
    }
   ],
   "source": [
    "print((x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norman-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584cbaa",
   "metadata": {},
   "source": [
    "## Training using LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model1 = LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "least-packaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f7a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.66%\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b01fd",
   "metadata": {},
   "source": [
    "Now we will fine tune our model by altering the parameters of our LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed4b10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LDA(solver='lsqr',shrinkage='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38655849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.97%\n"
     ]
    }
   ],
   "source": [
    "model2.fit(x_train, y_train)\n",
    "y_pred=model2.predict(x_test)\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0da0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import ShrunkCovariance\n",
    "model3 = LDA(solver='eigen',covariance_estimator=ShrunkCovariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e616849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "model3.fit(x_train, y_train)\n",
    "y_pred=model3.predict(x_test)\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6156b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LDA(solver='lsqr',shrinkage=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb46d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.72%\n"
     ]
    }
   ],
   "source": [
    "model4.fit(x_train, y_train)\n",
    "y_pred=model4.predict(x_test)\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9749561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a452f1",
   "metadata": {},
   "source": [
    "Let's check if our model is performing good or not by recording our own live audio and checking the live predicted emotion by our best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smart-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak Anything :\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Speak Anything :\")\n",
    "    test_rec = r.listen(source)\n",
    "    with open(\"test_rec.wav\", \"wb\") as f:\n",
    "        f.write(test_rec.get_wav_data())\n",
    "    print(model4.predict(np.array([extract_feature(\"test_rec.wav\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-musical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
